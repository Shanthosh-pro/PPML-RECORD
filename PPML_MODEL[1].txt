#14.Evaluate accuracy score of Logistic Regression with build in Dataset

from sklearn.model_selection import train_test_split
from sklearn.datasets import make_blobs
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
 
X, y = make_blobs(random_state=0)
 

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
 
clf_model = LogisticRegression()
clf_model.fit(X_train, y_train)
 
y_pred = clf_model.predict(X_test)
 
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
 
#13.Implement K Means using any dataset
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
 
x = [4, 5, 10, 4, 3, 11, 14, 6, 10, 12]
y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]
 
data = list(zip(x, y))
 
kmeans = KMeans(n_clusters=2)
kmeans.fit(data)
 
plt.scatter(x, y, c=kmeans.labels_)
plt.show()
 

#12. Implement Linear Regression using any data
 
import matplotlib.pyplot as plt
from scipy import stats
 
x = [5, 7, 8, 7, 2, 17, 2, 9, 4, 11, 12, 9, 6]
y = [99, 86, 87, 88, 111, 86, 103, 87, 94, 78, 77, 85, 86]
 
slope, intercept = stats.linregress(x, y)
 
def myfunc(x):
	return slope * x + intercept
 
mymodel = list(map(myfunc, x))
 
plt.scatter(x, y)
plt.plot(x, mymodel)
plt.show()

#11.Plot a line without  marker, with any marker, with multiple points and with default x points

import matplotlib.pyplot as plt
import numpy as np
 
xpoints = np.array([0, 6])
ypoints = np.array([0, 250])
plt.plot(xpoints, ypoints)
plt.show()
 

x = np.array([0, 5])
y = np.array([0, 25])
plt.plot(x, y, marker='p')
plt.show()
 
xpoints = np.array([1, 2, 6, 8])
ypoints = np.array([3, 8, 1, 10])
plt.plot(xpoints, ypoints)
plt.show()
 
ypoints = np.array([3, 8, 1, 10, 5, 7])
plt.plot(ypoints)
plt.show()
 

#10.Do the following wrangling operations, 1) Group by 2) Remove duplicates 3) Merge

import pandas as pd
 
car_selling_data = {
	'Brand': ['Maruti', 'Maruti', 'Maruti', 'Maruti', 'Hyundai', 'Hyundai', 'Toyota', 'Mahindra', 'Mahindra', 'Ford', 'Toyota', 'Ford'],
	'Year': [2010, 2011, 2009, 2013, 2010, 2011, 2011, 2010, 2013, 2010, 2010, 2011],
	'Sold': [6, 7, 9, 8, 3, 5, 2, 8, 7, 2, 4, 2]
}

df = pd.DataFrame(car_selling_data)
 
print("\nOriginal DataFrame:\n", df)
 
grouped = df.groupby("Year")
 
print("\nGroup by year 2010:\n", grouped.get_group(2010))
 
import pandas as pd
 
# Student data
student_data = {
	'Name': ['Amit', 'Praveen', 'Jagadesh', 'Rahul', 'Vishal', 'Suraj', 'Rishab', 'Sathish', 'Amit', 'Rahul', 'Praveen', 'Amit'],
	'Roll no': [123, 54, 29, 36, 59, 38, 12, 45, 34, 36, 54, 23],
	'Email': ['xxxx@gmail.com', 'xxxxxx@gmail.com', 'xxxxxx@gmail.com', 'xx@gmail.com', 'xxxxx@gmail.com', 'xxxxx@gmail.com', 'xxxx@gmail.com', 'xxxxx@gmail.com', 'xxxxxxx@gmail.com', 'xxxxxxxxxx@gmail.com', 'xxxxxxxxxx@gmail.com', 'xxxxxxxxxx@gmail.com']
}
 

df = pd.DataFrame(student_data)
 
print("\nOriginal DataFrame:\n", df)
 

non_duplicate = df[~df.duplicated('Roll no')]
 

print("\nRemoved duplicated rows:\n", non_duplicate)
 

details = pd.DataFrame({
	'ID': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],
	'NAME': ['Arun', 'Praveen', 'Harish', 'Pooja', 'Rahul', 'Naresh', 'Saurabh', 'Anush', 'Dinesh', 'Mohit'],
	'BRANCH': ['Mech', 'Mech', 'CSE', 'CSE', 'CSE', 'EEE', 'EEE', 'ECE', 'ECE', 'IT']
})
 
print("\nOriginal DataFrame 1:\n", details)
 

fees_status = pd.DataFrame({
	'ID': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],
	'PENDING': ['5000', '250', 'NIL', '9000', '15000', 'NIL', '4500', '1800', '250', 'NIL']
})
 
print("\nOriginal DataFrame:\n", fees_status)
 
print("\nMerged DataFrame:\n", pd.merge(details, fees_status, on='ID'))
 

#9) Support Vector Machine model usage for classification and regression

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error
import numpy as np

X, y = datasets.make_regression(n_samples=100, n_features=1, noise=10,random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
svm_regressor = SVR(kernel='linear', C=1.0)
svm_regressor.fit(X_train, y_train)
y_pred = svm_regressor.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

import matplotlib.pyplot as plt
plt.scatter(X_test, y_test, color='black', label='Actual Data')
plt.plot(X_test,y_pred, color='blue', linewidth=3, label='SVR Linear Regression')
plt.xlabel('Feature')
plt.ylabel('Target')
plt.title('Support Vector Machine Regression')
plt.legend()
plt.show()



#8) Multi Layer Neural Network For a Regression

def sigmoid(x):
    return 1 / (1 + np.exp(-x))
def sigmoid_derivative(x):
    return x * (1 - x)

def mse_loss(y_true, y_pred):
    return np.mean(np.square(y_true - y_pred))

inputs = np.array([[0.1, 0.2, 0.3],
[0.2, 0.3, 0.4],
[0.3, 0.4, 0.5],
[0.4, 0.5, 0.6]])

expected_output = np.array([[0.4],
[0.5],
[0.6],
[0.7]])

np.random.seed(1)

hidden_weights_1 = np.random.uniform(size=(inputs.shape[1], 4))
hidden_weights_2 = np.random.uniform(size=(4, 3))
output_weights = np.random.uniform(size=(3, 1))
 
lr = 0.1

for _ in range(10000):
   
    hidden_layer_output_1 = sigmoid(np.dot(inputs, hidden_weights_1))
    hidden_layer_output_2 = sigmoid(np.dot(hidden_layer_output_1, hidden_weights_2))
    predicted_output = np.dot(hidden_layer_output_2, output_weights) # Linear output
   
    error = expected_output - predicted_output
    d_predicted_output = error # Derivative for linear output
    error_hidden_layer_2 = d_predicted_output.dot(output_weights.T)
    d_hidden_layer_2 = error_hidden_layer_2 * sigmoid_derivative(hidden_layer_output_2)
    error_hidden_layer_1 = d_hidden_layer_2.dot(hidden_weights_2.T)
    d_hidden_layer_1 = error_hidden_layer_1 * sigmoid_derivative(hidden_layer_output_1)
    output_weights += hidden_layer_output_2.T.dot(d_predicted_output) * lr
    hidden_weights_2 += hidden_layer_output_1.T.dot(d_hidden_layer_2) * lr
    hidden_weights_1 += inputs.T.dot(d_hidden_layer_1) * lr
print('Final hidden weights 1: ', hidden_weights_1)
print('Final hidden weights 2: ', hidden_weights_2)
print('Final output weights: ', output_weights)
print('Output from neural network after 10,000 epochs: ', predicted_output)
hidden_weights_1 .shape
hidden_weights_2 .shape
output_weights.shape


#7) Create an Linear Regression Model Using Keras API

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)
tf.random.set_seed(42)

y = 4 + 3 * X + np.random.randn(100, 1)
 
model = tf.keras.Sequential ([
tf.keras.layers.Input(shape=(1,), name='input_layer'),
tf.keras.layers.Dense(1, name='output_layer') ])

model.compile(optimizer='sgd', loss='mse') # sgd stands for Stochastic Gradient Descent

model.summary()

history = model.fit(X, y, epochs=100, verbose=0)

plt.plot(history.history['loss'])
plt.title('Model Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error') 
plt.show()

weight, bias = model.layers[0].get_weights()
print("Weight:", weight[0, 0])
print("Bias:", bias[0])

X_new = np.array([[0], [2]])
predictions = model.predict(X_new)

for i in range(len(X_new)):
    print(f"Input: {X_new[i][0]}, Predicted Output: {predictions[i][0]}")



#6) Logistic Regression For the Generated Synthetic Data

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = (4 + 3 * X + np.random.randn(100, 1)) > 5 # Binary labels based on a threshold

X_tensor = tf.constant(X, dtype=tf.float32)
y_tensor = tf.constant(y, dtype=tf.float32)

W = tf.Variable(tf.random.normal(shape=(1, 1), mean=0.0, stddev=1.0), trainable=True)
b = tf.Variable(tf.zeros(shape=(1,)), trainable=True)

def sigmoid(x):
    return 1 / (1 + tf.exp(-x))

def binary_cross_entropy(y_true, y_pred):
    epsilon = 1e-15 # Small constant to avoid taking the log of zero
    y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon) # Clip to avoid numerical instability
    return -tf.reduce_mean(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))

learning_rate = 0.01
num_epochs = 100

for epoch in range(num_epochs):
    with tf.GradientTape() as tape:
        logits = tf.matmul(X_tensor, W) + b
        predictions = sigmoid(logits)
        loss = binary_cross_entropy(y_tensor, predictions)
gradients = tape.gradient(loss, [W, b])
W.assign_sub(learning_rate * gradients[0])
b.assign_sub(learning_rate * gradients[1])

if (epoch + 1) % 10 == 0:
    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.numpy()}')
plt.scatter(X, y, label='Data points')
x_values = np.linspace(0, 2, 100).reshape(-1, 1)
y_values = sigmoid(np.dot(x_values, W.numpy()) + b.numpy())
plt.plot(x_values, y_values, color='red', label='Logistic regression')
plt.xlabel('X')
plt.ylabel('Probability of Class 1')
plt.legend()
plt.show()



#5) Linear Regression Model for fitting an randomly Generated Data

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

X_tensor = tf.constant(X, dtype=tf.float32)
y_tensor = tf.constant(y, dtype=tf.float32)

W = tf.Variable(tf.random.normal(shape=(1, 1), mean=0.0, stddev=1.0), trainable=True)
b = tf.Variable(tf.zeros(shape=(1,)), trainable=True)

def linear_regression(x):
    return tf.matmul(x, W) + b

def mean_squared_error(y_true, y_pred):
    return tf.reduce_mean(tf.square(y_true - y_pred))

learning_rate = 0.01
num_epochs = 100

for epoch in range(num_epochs):
    with tf.GradientTape() as tape: 
        predictions= linear_regression(X_tensor)
        loss = mean_squared_error(y_tensor, predictions)

gradients = tape.gradient(loss, [W, b])
W.assign_sub(learning_rate * gradients[0])
b.assign_sub(learning_rate * gradients[1])
if (epoch + 1) % 10 == 0:
    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.numpy()}')
# Plot the data points and the linear regression line
plt.scatter(X, y, label='Data points')
plt.plot(X, linear_regression(X_tensor).numpy(), color='red', label='Linear regression') 
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()



#4) Vehicle Performance Analysis using pandas

import pandas as pd 
import numpy as np

data = {
    'Make': ['Toyota', 'Honda', 'Ford', 'Chevrolet', 'Nissan'],
    'Model': ['Camry', 'Civic', 'Fusion', 'Malibu', 'Altima'],
    'Fuel_Efficiency_MPG': [28, 36, 30, 32, 26],
    'Engine_Power_HP': [203, 158, 181, 163, 188],
    'Acceleration_0_60mph_s': [7.5, 8.2, 6.9, 8.0, 6.2],
    'Weight_kg': [1500, 1300, 1400, 1350, 1550]
}
car_performance_df = pd.DataFrame(data)

correlation_matrix = car_performance_df[['Fuel_Efficiency_MPG', 'Engine_Power_HP', 'Acceleration_0_60mph_s', 'Weight_kg']].corr()

car_performance_df['Performance_Index'] = (
    0.4 * car_performance_df['Fuel_Efficiency_MPG'] +
    0.4 * car_performance_df['Engine_Power_HP'] -
    0.2 * car_performance_df['Acceleration_0_60mph_s']
)

top_performing_cars = car_performance_df.nlargest(2, 'Performance_Index')

min_max_scaler = lambda x: (x - np.min(x)) / (np.max(x) - np.min(x))
car_performance_df['Weight_normalized'] = car_performance_df[['Weight_kg']].apply(min_max_scaler)

print("Correlation Matrix:")
print(correlation_matrix)

print("\nDataFrame with Performance Index:")
print(car_performance_df[['Make', 'Model', 'Performance_Index']])

print("\nTop Performing Cars:")
print(top_performing_cars[['Make', 'Model', 'Performance_Index']])

print("\nDataFrame with Weight Adjustment:")
print(car_performance_df[['Make', 'Model', 'Weight_kg', 'Weight_normalized']])




#3) Write a program to find minimum of function f (x, y)=x^2 + y^2 + 4 using gradient desent algorithm

import numpy as np
def gradient_descent(f, initial_point, learning_rate, iterations): 
    point = initial_point.copy() 
    history = []
    
    for _ in  range (iterations):
        gradient = np.array([2 * point[0], 2 * point[1]])
        point -= learning_rate * gradient
        value= f(point [0], point[1])

        history.append((point.copy(), value))
        print("Values of points and objective function value are: () and {}".format(point, value))
    return point, history
def objective_function(x, y): 
    return x**2 + y**2 + 4

initial_point = np.array([1.0, 1.0])
learning_rate = 0.1
iterations = 100
minimum_point, history = gradient_descent(objective_function, initial_point, learning_rate, iterations)
print("Minimum point (x, y):", minimum_point) 
print("Minimum value of the function:", objective_function(*minimum_point))


#2) Write a simple Python code to generate random values and then compute
their sigmoid and tanh (hyperbolic tangent) values using NumPy. Plot the
values.

import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

random_values = np.random.randn(10) 

sigmoid_values = sigmoid(random_values)
tanh_values = tanh(random_values)

indices = np.arange(len(random_values))

plt.figure(figsize=(14,6))

plt.subplot(1, 2, 1)
plt.scatter(indices, sigmoid_values, color='blue', label='Sigmoid Values')
plt.plot(indices, sigmoid_values, color='lightblue', linestyle='--')
plt.title('Sigmoid Function')
plt.xlabel('Index')
plt.ylabel('Sigmoid Value')
plt.grid(True)
plt.legend()

plt.subplot(1, 2, 2)
plt.scatter(indices, tanh_values, color='red', label='Tanh Values')
plt.plot(indices, tanh_values, color='pink', linestyle='--')
plt.title('Tanh Function')
plt.xlabel('Index')
plt.ylabel('Tanh Value')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()



#1)Calculating values of random data using NumPy for mathematical formulas
#a)Euclidean distance between two points b) Dot Product of two Vectors
#c)Solving a System of Linear Equations

import numpy as np

def euclidean_distance(p, q):
    return np.sqrt(np.sum((q - p) ** 2))

p = np.array([1, 2])
q = np.array([4, 6])
distance = euclidean_distance(p, q)
print("Output for Calculating the Euclidean Distance Between Two Points is:", distance)

A = np.array([1, 3, -5])
B = np.array([4, -2, -1])
dot_product = np.dot(A, B)
print("Output for dot product of two vectors A and B is:", dot_product)

A = np.array([[3, 1], [1, 2]])
b = np.array([9, 8])

x = np.linalg.solve(A, b)
print("Output solution of System of Linear Equations is:", x)

