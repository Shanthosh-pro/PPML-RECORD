14.Evaluate accuracy score of Logistic Regression with build in Dataset

from sklearn.model_selection import train_test_split
from sklearn.datasets import make_blobs
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
 
# Generating synthetic data
X, y = make_blobs(random_state=0)
 
# Splitting data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
 
# Creating and training logistic regression model
clf_model = LogisticRegression()
clf_model.fit(X_train, y_train)
 
# Making predictions on the test data
y_pred = clf_model.predict(X_test)
 
# Evaluating model accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
 


13.Implement K Means using any dataset

import matplotlib.pyplot as plt
from scipy import stats
 
x = [5, 7, 8, 7, 2, 17, 2, 9, 4, 11, 12, 9, 6]
y = [99, 86, 87, 88, 111, 86, 103, 87, 94, 78, 77, 85, 86]
 
# Calculate linear regression
slope, intercept = stats.linregress(x, y)
 
# Define linear regression function
def myfunc(x):
	return slope * x + intercept
 
# Generate model values
mymodel = list(map(myfunc, x))
 
# Plot original data points and regression line
plt.scatter(x, y)
plt.plot(x, mymodel)
plt.show()


12. Implement Linear Regression using any data
 
import matplotlib.pyplot as plt
from scipy import stats
 
x = [5, 7, 8, 7, 2, 17, 2, 9, 4, 11, 12, 9, 6]
y = [99, 86, 87, 88, 111, 86, 103, 87, 94, 78, 77, 85, 86]
 
# Calculate linear regression
slope, intercept = stats.linregress(x, y)
 
# Define linear regression function
def myfunc(x):
	return slope * x + intercept
 
# Generate model values
mymodel = list(map(myfunc, x))
 
# Plot original data points and regression line
plt.scatter(x, y)
plt.plot(x, mymodel)
plt.show()


11.Plot a line without  marker, with any marker, with multiple points and with default x points


import matplotlib.pyplot as plt
import numpy as np
 
# Plot with line
xpoints = np.array([0, 6])
ypoints = np.array([0, 250])
plt.plot(xpoints, ypoints)
plt.show()
 
# Points without line
x = np.array([0, 5])
y = np.array([0, 25])
plt.plot(x, y, marker='p')
plt.show()
 
# Multiple points
xpoints = np.array([1, 2, 6, 8])
ypoints = np.array([3, 8, 1, 10])
plt.plot(xpoints, ypoints)
plt.show()
 
# Default x points
ypoints = np.array([3, 8, 1, 10, 5, 7])
plt.plot(ypoints)
plt.show()
 

10.Do the following wrangling operations, 1) Group by 2) Remove duplicates 3) Merge

import pandas as pd
 
# Creating data
car_selling_data = {
	'Brand': ['Maruti', 'Maruti', 'Maruti', 'Maruti', 'Hyundai', 'Hyundai', 'Toyota', 'Mahindra', 'Mahindra', 'Ford', 'Toyota', 'Ford'],
	'Year': [2010, 2011, 2009, 2013, 2010, 2011, 2011, 2010, 2013, 2010, 2010, 2011],
	'Sold': [6, 7, 9, 8, 3, 5, 2, 8, 7, 2, 4, 2]
}
 
# Creating DataFrame
df = pd.DataFrame(car_selling_data)
 
print("\nOriginal DataFrame:\n", df)
 
# Grouping the data by year
grouped = df.groupby("Year")
 
print("\nGroup by year 2010:\n", grouped.get_group(2010))
 
import pandas as pd
 
# Student data
student_data = {
	'Name': ['Amit', 'Praveen', 'Jagadesh', 'Rahul', 'Vishal', 'Suraj', 'Rishab', 'Sathish', 'Amit', 'Rahul', 'Praveen', 'Amit'],
	'Roll no': [123, 54, 29, 36, 59, 38, 12, 45, 34, 36, 54, 23],
	'Email': ['xxxx@gmail.com', 'xxxxxx@gmail.com', 'xxxxxx@gmail.com', 'xx@gmail.com', 'xxxxx@gmail.com', 'xxxxx@gmail.com', 'xxxx@gmail.com', 'xxxxx@gmail.com', 'xxxxxxx@gmail.com', 'xxxxxxxxxx@gmail.com', 'xxxxxxxxxx@gmail.com', 'xxxxxxxxxx@gmail.com']
}
 
# Creating DataFrame
df = pd.DataFrame(student_data)
 
print("\nOriginal DataFrame:\n", df)
 
# Removing duplicate rows based on 'Roll no'
non_duplicate = df[~df.duplicated('Roll no')]
 
# Printing non-duplicate values
print("\nRemoved duplicated rows:\n", non_duplicate)
 
# Creating DataFrame
details = pd.DataFrame({
	'ID': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],
	'NAME': ['Arun', 'Praveen', 'Harish', 'Pooja', 'Rahul', 'Naresh', 'Saurabh', 'Anush', 'Dinesh', 'Mohit'],
	'BRANCH': ['Mech', 'Mech', 'CSE', 'CSE', 'CSE', 'EEE', 'EEE', 'ECE', 'ECE', 'IT']
})
 
print("\nOriginal DataFrame 1:\n", details)
 
# Creating DataFrame
fees_status = pd.DataFrame({
	'ID': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],
	'PENDING': ['5000', '250', 'NIL', '9000', '15000', 'NIL', '4500', '1800', '250', 'NIL']
})
 
print("\nOriginal DataFrame:\n", fees_status)
 
# Merging DataFrames
print("\nMerged DataFrame:\n", pd.merge(details, fees_status, on='ID'))
 

9) Support Vector Machine model usage for classification and regression

Program 

#support vector machine in scikit learn library
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error
import numpy as np
# Generate some example data
X, y = datasets.make_regression(n_samples=100, n_features=1, noise=10,random_state=42)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)
# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Create SVM Regression model
svm_regressor = SVR(kernel='linear', C=1.0)
# Train the SVM Regression model
svm_regressor.fit(X_train, y_train)
# Make predictions on the test set
y_pred = svm_regressor.predict(X_test)
# Evaluate the performance using Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
# Visualize the regression line
import matplotlib.pyplot as plt
plt.scatter(X_test, y_test, color='black', label='Actual Data')
plt.plot(X_test,y_pred, color='blue', linewidth=3, label='SVR Linear Regression')
plt.xlabel('Feature')
plt.ylabel('Target')
plt.title('Support Vector Machine Regression')
plt.legend()
plt.show()



8) Multi Layer Neural Network For a Regression

Program

# multi-layer neural network example for a regression
import numpy as np
# Define sigmoid activation function and its derivative for hidden layers 
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
def sigmoid_derivative(x):
    return x * (1 - x)
# Mean Squared Error Loss Function
def mse_loss(y_true, y_pred):
    return np.mean(np.square(y_true - y_pred))
# Example input data: 3 features (This should be replaced with your data) 
inputs = np.array([[0.1, 0.2, 0.3],
[0.2, 0.3, 0.4],
[0.3, 0.4, 0.5],
[0.4, 0.5, 0.6]])
# Example output data: 1 output (This should be replaced with your data)
expected_output = np.array([[0.4],
[0.5],
[0.6],
[0.7]])
# Seed for random number generation for reproducibility
np.random.seed(1)
# Initialize weights randomly with mean 0
hidden_weights_1 = np.random.uniform(size=(inputs.shape[1], 4))
hidden_weights_2 = np.random.uniform(size=(4, 3))
output_weights = np.random.uniform(size=(3, 1))
# Learning rate 
lr = 0.1
# Training algorithm
for _ in range(10000):
    # Forward pass
    hidden_layer_output_1 = sigmoid(np.dot(inputs, hidden_weights_1))
    hidden_layer_output_2 = sigmoid(np.dot(hidden_layer_output_1, hidden_weights_2))
    predicted_output = np.dot(hidden_layer_output_2, output_weights) # Linear output
    # Backward pass
    error = expected_output - predicted_output
    d_predicted_output = error # Derivative for linear output
    error_hidden_layer_2 = d_predicted_output.dot(output_weights.T)
    d_hidden_layer_2 = error_hidden_layer_2 * sigmoid_derivative(hidden_layer_output_2)
    error_hidden_layer_1 = d_hidden_layer_2.dot(hidden_weights_2.T)
    d_hidden_layer_1 = error_hidden_layer_1 * sigmoid_derivative(hidden_layer_output_1)
    # Updating Weights
    output_weights += hidden_layer_output_2.T.dot(d_predicted_output) * lr
    hidden_weights_2 += hidden_layer_output_1.T.dot(d_hidden_layer_2) * lr
    hidden_weights_1 += inputs.T.dot(d_hidden_layer_1) * lr
print('Final hidden weights 1: ', hidden_weights_1)
print('Final hidden weights 2: ', hidden_weights_2)
print('Final output weights: ', output_weights)
print('Output from neural network after 10,000 epochs: ', predicted_output)
hidden_weights_1 .shape
hidden_weights_2 .shape
output_weights.shape


7) Create an Linear Regression Model Using Keras API

Program 

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
# Generate random data
np.random.seed(42)
tf.random.set_seed(42)
# Generate random data
y = 4 + 3 * X + np.random.randn(100, 1)
# Create a linear model 
model = tf.keras.Sequential ([
tf.keras.layers.Input(shape=(1,), name='input_layer'),
tf.keras.layers.Dense(1, name='output_layer') ])
# Compile the model
model.compile(optimizer='sgd', loss='mse') # sgd stands for Stochastic Gradient Descent
# Display the model summary
model.summary()
# Train the model
history = model.fit(X, y, epochs=100, verbose=0)
# Plot the training loss
plt.plot(history.history['loss'])
plt.title('Model Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error') 
plt.show()
# Get the learned weight and bias
weight, bias = model.layers[0].get_weights()
print("Weight:", weight[0, 0])
print("Bias:", bias[0])
# Make predictions
X_new = np.array([[0], [2]])
predictions = model.predict(X_new)
# Display predictions
for i in range(len(X_new)):
    print(f"Input: {X_new[i][0]}, Predicted Output: {predictions[i][0]}")



6) Logistic Regression For the Generated Synthetic Data

Program

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
# Generate synthetic data for binary classification
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = (4 + 3 * X + np.random.randn(100, 1)) > 5 # Binary labels based on a threshold
# Convert data to TensorFlow tensors
X_tensor = tf.constant(X, dtype=tf.float32)
y_tensor = tf.constant(y, dtype=tf.float32)
# Initialize variables (weights and bias)
W = tf.Variable(tf.random.normal(shape=(1, 1), mean=0.0, stddev=1.0), trainable=True)
b = tf.Variable(tf.zeros(shape=(1,)), trainable=True)
# Logistic sigmoid function
def sigmoid(x):
    return 1 / (1 + tf.exp(-x))
# Binary cross-entropy loss function
def binary_cross_entropy(y_true, y_pred):
    epsilon = 1e-15 # Small constant to avoid taking the log of zero
    y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon) # Clip to avoid numerical instability
    return -tf.reduce_mean(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))
# Set hyperparameters
learning_rate = 0.01
num_epochs = 100
# Optimization using gradient descent
for epoch in range(num_epochs):
    with tf.GradientTape() as tape:
        logits = tf.matmul(X_tensor, W) + b
        predictions = sigmoid(logits)
        loss = binary_cross_entropy(y_tensor, predictions)
# Calculate gradients
gradients = tape.gradient(loss, [W, b])
# Update weights and bias using gradient descent
W.assign_sub(learning_rate * gradients[0])
b.assign_sub(learning_rate * gradients[1])
# Print the loss every 10 epochs
if (epoch + 1) % 10 == 0:
    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.numpy()}')
# Plot the data points and the logistic regression curve
plt.scatter(X, y, label='Data points')
x_values = np.linspace(0, 2, 100).reshape(-1, 1)
y_values = sigmoid(np.dot(x_values, W.numpy()) + b.numpy())
plt.plot(x_values, y_values, color='red', label='Logistic regression')
plt.xlabel('X')
plt.ylabel('Probability of Class 1')
plt.legend()
plt.show()



5) Linear Regression Model for fitting an randomly Generated Data

Program

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
# Generate some random data for a linear regression problem 
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)
# Convert data to TensorFlow tensors
X_tensor = tf.constant(X, dtype=tf.float32)
y_tensor = tf.constant(y, dtype=tf.float32)
# Initialize variables (weights and bias)
W = tf.Variable(tf.random.normal(shape=(1, 1), mean=0.0, stddev=1.0), trainable=True)
b = tf.Variable(tf.zeros(shape=(1,)), trainable=True)
# Define the linear regression model
def linear_regression(x):
    return tf.matmul(x, W) + b
# Define the mean squared error loss function
def mean_squared_error(y_true, y_pred):
    return tf.reduce_mean(tf.square(y_true - y_pred))
# Set hyperparameters
learning_rate = 0.01
num_epochs = 100
# Optimization using gradient descent
for epoch in range(num_epochs):
    with tf.GradientTape() as tape: 
        predictions= linear_regression(X_tensor)
        loss = mean_squared_error(y_tensor, predictions)
# Calculate gradients
gradients = tape.gradient(loss, [W, b])
# Update weights and bias using gradient descent
W.assign_sub(learning_rate * gradients[0])
b.assign_sub(learning_rate * gradients[1])
# Print the loss every 10 epochs
if (epoch + 1) % 10 == 0:
    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.numpy()}')
# Plot the data points and the linear regression line
plt.scatter(X, y, label='Data points')
plt.plot(X, linear_regression(X_tensor).numpy(), color='red', label='Linear regression') 
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()



4) Vehicle Performance Analysis using pandas

Program

import pandas as pd 
import numpy as np

# Creating a hypothetical dataset
data = {
    'Make': ['Toyota', 'Honda', 'Ford', 'Chevrolet', 'Nissan'],
    'Model': ['Camry', 'Civic', 'Fusion', 'Malibu', 'Altima'],
    'Fuel_Efficiency_MPG': [28, 36, 30, 32, 26],
    'Engine_Power_HP': [203, 158, 181, 163, 188],
    'Acceleration_0_60mph_s': [7.5, 8.2, 6.9, 8.0, 6.2],
    'Weight_kg': [1500, 1300, 1400, 1350, 1550]
}
car_performance_df = pd.DataFrame(data)

# Task 1: Correlation Analysis
correlation_matrix = car_performance_df[['Fuel_Efficiency_MPG', 'Engine_Power_HP', 'Acceleration_0_60mph_s', 'Weight_kg']].corr()

# Task 2: Performance Index Calculation
car_performance_df['Performance_Index'] = (
    0.4 * car_performance_df['Fuel_Efficiency_MPG'] +
    0.4 * car_performance_df['Engine_Power_HP'] -
    0.2 * car_performance_df['Acceleration_0_60mph_s']
)

# Task 3: Top Performing Cars
top_performing_cars = car_performance_df.nlargest(2, 'Performance_Index')

# Task 4: Weight Adjustment (Min-Max scaling)
min_max_scaler = lambda x: (x - np.min(x)) / (np.max(x) - np.min(x))
car_performance_df['Weight_normalized'] = car_performance_df[['Weight_kg']].apply(min_max_scaler)

# Output
print("Correlation Matrix:")
print(correlation_matrix)

print("\nDataFrame with Performance Index:")
print(car_performance_df[['Make', 'Model', 'Performance_Index']])

print("\nTop Performing Cars:")
print(top_performing_cars[['Make', 'Model', 'Performance_Index']])

print("\nDataFrame with Weight Adjustment:")
print(car_performance_df[['Make', 'Model', 'Weight_kg', 'Weight_normalized']])




3) Write a program to find minimum of function f (x, y)=x^2 + y^2 + 4 using gradient desent algorithm

Program

import numpy as np
def gradient_descent(f, initial_point, learning_rate, iterations): 
    point = initial_point.copy() 
    history = []
    
    for _ in  range (iterations):
        # Compute gradient of the function at the current point 
        gradient = np.array([2 * point[0], 2 * point[1]])
        # Update point using gradient descent update rule 
        point -= learning_rate * gradient
        # Calculate the value of the function at the updated point
        value= f(point [0], point[1])
        # Save the history of points and function values 
        history.append((point.copy(), value))
        print("Values of points and objective function value are: () and {}".format(point, value))
    return point, history
def objective_function(x, y): 
    return x**2 + y**2 + 4
# Define initial point, Learning rate, and number of iterations
initial_point = np.array([1.0, 1.0])
learning_rate = 0.1
iterations = 100
# Run gradient descent
minimum_point, history = gradient_descent(objective_function, initial_point, learning_rate, iterations)
print("Minimum point (x, y):", minimum_point) 
print("Minimum value of the function:", objective_function(*minimum_point))


2) Write a simple Python code to generate random values and then compute
their sigmoid and tanh (hyperbolic tangent) values using NumPy. Plot the
values.

Program 

import numpy as np
import matplotlib.pyplot as plt

# Define the sigmoid function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Define the tanh function
def tanh(x):
    return np.tanh(x)

# Generate a random array of values using numpy
random_values = np.random.randn(10)  # Generate 10 random values from a standard normal distribution

# Calculate the sigmoid and tanh (hyperbolic tangent) of these random values
sigmoid_values = sigmoid(random_values)
tanh_values = tanh(random_values)

# Generate indices for x-axis
indices = np.arange(len(random_values))

# Plotting
plt.figure(figsize=(14,6))

# Plot for sigmoid values
plt.subplot(1, 2, 1)
plt.scatter(indices, sigmoid_values, color='blue', label='Sigmoid Values')
plt.plot(indices, sigmoid_values, color='lightblue', linestyle='--')
plt.title('Sigmoid Function')
plt.xlabel('Index')
plt.ylabel('Sigmoid Value')
plt.grid(True)
plt.legend()

# Plot for Tanh Values
plt.subplot(1, 2, 2)
plt.scatter(indices, tanh_values, color='red', label='Tanh Values')
plt.plot(indices, tanh_values, color='pink', linestyle='--')
plt.title('Tanh Function')
plt.xlabel('Index')
plt.ylabel('Tanh Value')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()



1)Calculating values of random data using NumPy for mathematical formulas
a)Euclidean distance between two points b) Dot Product of two Vectors
c)Solving a System of Linear Equations

Program

import numpy as np

# Function to calculate Euclidean distance
def euclidean_distance(p, q):
    return np.sqrt(np.sum((q - p) ** 2))

# Example usage
p = np.array([1, 2])
q = np.array([4, 6])
distance = euclidean_distance(p, q)
print("Output for Calculating the Euclidean Distance Between Two Points is:", distance)

# Calculating the Dot Product of Two Vectors
A = np.array([1, 3, -5])
B = np.array([4, -2, -1])
dot_product = np.dot(A, B)
print("Output for dot product of two vectors A and B is:", dot_product)

# Solving a System of Linear Equations
# Coefficients matrix A and result vector b
A = np.array([[3, 1], [1, 2]])
b = np.array([9, 8])

# Solve for x
x = np.linalg.solve(A, b)
print("Output solution of System of Linear Equations is:", x)

